---
title: "Data Acquisition"
author: "Sadi Tomtulu"
---

# WEBSCRAPING ----

# 1.0 LIBRARIES ----

```{r}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing
```

# Example

```{r}
library(RSQLite)

con <- dbConnect(drv = SQLite(), dbname = 'C:/Users/sedit/Desktop/SadiR/ss23-bdsb-Sadtom98/content/01_journal/02_data_acquisition_/Chinook_Sqlite.sqlite')
dbListTables(con)

tbl(con, "Album")
album_tbl <- tbl(con, "Album") %>% collect()
dbDisconnect(con)
con
library(httr)

resp <- GET("https://swapi.dev/api/people/1/")
resp
```



```{r}
url <- "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
# use that URL to scrape the S&P 500 table using rvest

sp_500 <- url %>%
          # read the HTML from the webpage
          read_html() %>%
          # Get the nodes with the id
          html_nodes(css = "#constituents") %>%
          # html_nodes(xpath = "//*[@id='constituents']"") %>% 
          # Extract the table and turn the list into a tibble
          html_table() %>% 
          .[[1]] %>% 
          as_tibble()
bike_data_lst <- fromJSON("C:/Users/sedit/Desktop/SadiR/ss23-bdsb-Sadtom98/content/01_journal/02_data_acquisition_/bike_data.json")


```

# Challange 1st
```{r}
weather <- GET("https://api.open-meteo.com/v1/forecast?latitude=52.52&longitude=13.41&hourly=temperature_2m")
weather
hourly <- rawToChar(weather$content) %>% fromJSON()
hour <- hourly$hourly$time
temp <- hourly$hourly$temperature_2m
df
df <- data.frame(hour, temp)
df <- df %>% mutate(day = day(df$hour))
p <- df %>% ggplot() + geom_point(aes(x = hour , y = temp))
p + guides(x = guide_axis(check.overlap = TRUE, n.dodge = 1)) + scale_x_discrete(breaks = waiver()) + theme(axis.text.x = element_text(angle=45))
```


# 1.1 COLLECT PRODUCT FAMILIES ----

```{r}
url_home <- "https://www.rosebikes.de/fahrräder"
# xopen(url_home) # Open links directly from RStudio to inspect them

# Read in the HTML for the entire webpage
html_home <- read_html(url_home)

category <- html_home %>%
  html_nodes(css = ".catalog-navigation__link")
links <- sapply(category, function(x) {x %>% html_attr("href")})
links <- links[1:9] %>%
  enframe(name = "position", value = "subdirectory") %>%
  mutate(
    url = glue("https://www.rosebikes.de{subdirectory}"))  %>%
  distinct(url)
links
```

# Challange 2, extracting data from website

```{r}
databike <- function(url) {
  html_bike_cat <- read_html(url)
  
  pricings <- html_nodes(html_bike_cat, css = '.catalog-category-bikes__price-title') %>% 
    html_text(trim=TRUE) %>%              
    str_replace_all(" ","") %>%
    str_replace_all("ab", "") %>%
    str_replace_all("€", "") %>%
    str_replace_all("\n", "") %>%
    str_replace_all("\\.", "") %>%
    str_replace_all(",", "\\.") %>%
    iconv('utf-8', 'ascii', sub='') %>%
    as.numeric()
  
  names <- html_nodes(html_bike_cat, xpath = '//basic-headline/h4') %>% 
    html_text() %>%
    str_replace_all("\n", "") %>%
    str_to_title()
  
  categories <- rep(url %>% str_replace_all("https://www.rosebikes.de/fahrräder/", ""), 
              each=length(names)) %>%
    str_to_title()
  
  return(list("pricings" = pricings, "names" = names, "categories" = categories))
}

data <- databike(links$url[1])
bike_data <- tibble(bike.type = data$categories,
                    bike.name = data$names,
                    bike.price = as.numeric(data$pricings))
for (i in 2:9) {
  data <- databike(links$url[i])
  
  bike_data <- bike_data %>% add_row(bike.type = data$categories,
                                     bike.name = data$names,
                                     bike.price = as.numeric(data$pricings))
}

head(bike_data, 10)
```


